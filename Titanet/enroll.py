import torch
import torchaudio
import torch.nn.functional as F
import numpy as np
from nemo.collections.asr.models import EncDecSpeakerLabelModel

# ===== LOAD TITANET-S =====
device = "cuda" if torch.cuda.is_available() else "cpu"
model = EncDecSpeakerLabelModel.restore_from("titanet-l.nemo", map_location=device)
model = model.to(device)
model.eval()

# ===== HÃ€M Láº¤Y EMBEDDING =====
@torch.no_grad()
def get_embedding_from_wav(wav, sr):
    # Äáº£m báº£o mono
    if wav.shape[0] > 1:
        wav = wav[:1, :]
    
    # Resample vá» 16kHz
    if sr != 16000:
        wav = torchaudio.functional.resample(wav, sr, 16000)
        sr = 16000

    # Chuáº©n hÃ³a amplitude
    wav = wav / (wav.abs().max() + 1e-7)
    
    # Chuáº©n input: [B, T]
    audio_signal = wav.squeeze(0).unsqueeze(0).to(device)   # [1, T]
    audio_length = torch.tensor([audio_signal.shape[-1]], device=device)

    emb, _ = model.forward(
        input_signal=audio_signal,
        input_signal_length=audio_length
    )
    
    # L2 normalize embedding
    emb = F.normalize(emb, p=2, dim=1)
    return emb.squeeze().cpu().numpy()


def get_embedding(file, start=0, end=None):
    wav, sr = torchaudio.load(file)
    
    # Cáº¯t Ä‘oáº¡n audio náº¿u cáº§n
    if end:
        wav = wav[:, int(start*sr): int(end*sr)]
    elif start > 0:
        wav = wav[:, int(start*sr):]
    
    return get_embedding_from_wav(wav, sr)

# ===== 1. ENROLL GIá»ŒNG Báº N =====
enroll_file = "Viá»‡t Anh_24.9.wav"   # Ä‘á»•i thÃ nh file giá»ng báº¡n
print(f"ğŸ”„ Äang táº¡o embedding tá»« file: {enroll_file}")

# Kiá»ƒm tra Ä‘á»™ dÃ i file
wav_check, sr_check = torchaudio.load(enroll_file)
duration = wav_check.shape[1] / sr_check
print(f"ğŸ“ Äá»™ dÃ i file: {duration:.2f}s")

# Láº¥y embedding tá»« Ä‘oáº¡n Ä‘áº§u (tá»‘i Ä‘a 30s Ä‘á»ƒ trÃ¡nh quÃ¡ dÃ i)
max_duration = min(30, duration - 1)  # Láº¥y tá»‘i Ä‘a 30s, trÃ¡nh pháº§n cuá»‘i
enroll_emb = get_embedding(enroll_file, 1, max_duration)  # Báº¯t Ä‘áº§u tá»« giÃ¢y thá»© 1
np.save("enroll_emb.npy", enroll_emb)
print(f"âœ… ÄÃ£ lÆ°u embedding giá»ng báº¡n vÃ o enroll_emb.npy (shape: {enroll_emb.shape})")
print(f"ğŸ“Š Norm cá»§a embedding: {np.linalg.norm(enroll_emb):.3f}")

# ===== 2. VERIFY Vá»šI FILE Má»šI (CÃ“ 2 SPEAKERS) =====
verify_file = "conversation-test.wav"   # Ä‘á»•i thÃ nh file há»™i thoáº¡i
print(f"\nğŸ” Äang phÃ¢n tÃ­ch file: {verify_file}")

wav, sr = torchaudio.load(verify_file)
verify_duration = wav.shape[1] / sr
print(f"ğŸ“ Äá»™ dÃ i file verify: {verify_duration:.2f}s")

# Preprocessing nháº¥t quÃ¡n
if wav.shape[0] > 1:
    wav = wav[:1, :]
if sr != 16000:
    wav = torchaudio.functional.resample(wav, sr, 16000)
    sr = 16000

# ===== Silero VAD =====
silero_model, utils = torch.hub.load(
    repo_or_dir="snakers4/silero-vad",
    model="silero_vad",
    force_reload=False,
    onnx=False
)
(get_speech_timestamps, _, _, _, _) = utils
segments = get_speech_timestamps(
    wav.squeeze(), silero_model, sampling_rate=sr,
    threshold=0.4, min_speech_duration_ms=250, min_silence_duration_ms=150
)

print(f"ğŸ” VAD phÃ¡t hiá»‡n {len(segments)} Ä‘oáº¡n nÃ³i")

# ===== So sÃ¡nh tá»«ng Ä‘oáº¡n vá»›i giá»ng báº¡n =====
cosine_scores = []
same_speaker_segments = []
different_speaker_segments = []

for i, seg in enumerate(segments, 1):
    start_s = seg["start"] / sr
    end_s   = seg["end"] / sr
    duration = end_s - start_s
    
    # Bá» qua Ä‘oáº¡n quÃ¡ ngáº¯n
    if duration < 0.5:
        print(f"Äoáº¡n {i}: {start_s:.2f}s â€“ {end_s:.2f}s | â­ï¸ Bá» qua (quÃ¡ ngáº¯n: {duration:.2f}s)")
        continue
    
    seg_wav = wav[:, seg["start"]:seg["end"]]
    emb = get_embedding_from_wav(seg_wav, sr)

    # TÃ­nh cosine similarity vá»›i embedding Ä‘Ã£ normalize
    cos_sim = F.cosine_similarity(
        torch.tensor(enroll_emb).unsqueeze(0),
        torch.tensor(emb).unsqueeze(0)
    ).item()
    
    cosine_scores.append(cos_sim)
    
    # Lowered threshold vÃ  thÃªm thÃ´ng tin chi tiáº¿t
    if cos_sim > 0.4:  # Threshold tháº¥p hÆ¡n
        label = "âœ… CÃ³ thá»ƒ lÃ  giá»ng báº¡n"
        same_speaker_segments.append((i, cos_sim))
    else:
        label = "âŒ NgÆ°á»i khÃ¡c"
        different_speaker_segments.append((i, cos_sim))
    
    print(f"Äoáº¡n {i}: {start_s:.2f}s â€“ {end_s:.2f}s ({duration:.1f}s) | Cosine={cos_sim:.3f} | {label}")

# ===== THá»NG KÃŠ =====
if cosine_scores:
    print(f"\nğŸ“Š THá»NG KÃŠ:")
    print(f"   â€¢ Sá»‘ Ä‘oáº¡n phÃ¢n tÃ­ch: {len(cosine_scores)}")
    print(f"   â€¢ Cosine trung bÃ¬nh: {np.mean(cosine_scores):.3f}")
    print(f"   â€¢ Cosine cao nháº¥t: {max(cosine_scores):.3f}")
    print(f"   â€¢ Cosine tháº¥p nháº¥t: {min(cosine_scores):.3f}")
    print(f"   â€¢ Äá»™ lá»‡ch chuáº©n: {np.std(cosine_scores):.3f}")
    
    print(f"\nğŸ¯ Káº¾T QUáº¢ PHÃ‚N LOáº I:")
    print(f"   â€¢ CÃ³ thá»ƒ lÃ  giá»ng báº¡n: {len(same_speaker_segments)} Ä‘oáº¡n")
    print(f"   â€¢ NgÆ°á»i khÃ¡c: {len(different_speaker_segments)} Ä‘oáº¡n")
