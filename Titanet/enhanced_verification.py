#!/usr/bin/env python3
"""
Enhanced Speaker Verification with Dynamic Threshold
"""
import torch
import torchaudio
import torch.nn.functional as F
import numpy as np
from nemo.collections.asr.models import EncDecSpeakerLabelModel

# ===== LOAD TITANET-L =====
device = "cuda" if torch.cuda.is_available() else "cpu"
model = EncDecSpeakerLabelModel.restore_from("titanet-l.nemo", map_location=device)
model = model.to(device)
model.eval()

# ===== HÃ€M Láº¤Y EMBEDDING =====
@torch.no_grad()
def get_embedding_from_wav(wav, sr):
    # Äáº£m báº£o mono
    if wav.shape[0] > 1:
        wav = wav[:1, :]
    
    # Resample vá» 16kHz
    if sr != 16000:
        wav = torchaudio.functional.resample(wav, sr, 16000)
        sr = 16000

    # Chuáº©n hÃ³a amplitude
    wav = wav / (wav.abs().max() + 1e-7)
    
    # Chuáº©n input: [B, T]
    audio_signal = wav.squeeze(0).unsqueeze(0).to(device)   # [1, T]
    audio_length = torch.tensor([audio_signal.shape[-1]], device=device)

    emb, _ = model.forward(
        input_signal=audio_signal,
        input_signal_length=audio_length
    )
    
    # L2 normalize embedding
    emb = F.normalize(emb, p=2, dim=1)
    return emb.squeeze().cpu().numpy()


def get_embedding(file, start=0, end=None):
    wav, sr = torchaudio.load(file)
    
    # Cáº¯t Ä‘oáº¡n audio náº¿u cáº§n
    if end:
        wav = wav[:, int(start*sr): int(end*sr)]
    elif start > 0:
        wav = wav[:, int(start*sr):]
    
    return get_embedding_from_wav(wav, sr)

# ===== KIá»‚M TRA SILERO VAD VERSION =====
def check_silero_version():
    """Kiá»ƒm tra vÃ  hiá»ƒn thá»‹ thÃ´ng tin Silero VAD version"""
    try:
        model, utils = torch.hub.load(
            repo_or_dir="snakers4/silero-vad",
            model="silero_vad",
            force_reload=False,
            onnx=False
        )
        
        # Check v6 features
        has_reset_states = hasattr(model, 'reset_states')
        has_vad_iterator = len(utils) >= 5
        
        print(f"ğŸ” Silero VAD Info:")
        print(f"   â€¢ Model loaded: âœ…")
        print(f"   â€¢ Has reset_states (v6): {'âœ…' if has_reset_states else 'âŒ'}")
        print(f"   â€¢ Has VADIterator: {'âœ…' if has_vad_iterator else 'âŒ'}")
        print(f"   â€¢ Utils functions: {len(utils)}")
        
        if has_reset_states and has_vad_iterator:
            print(f"   â€¢ Version: v6 compatible âœ…")
            return True
        else:
            print(f"   â€¢ Version: Older version âš ï¸")
            return False
            
    except Exception as e:
        print(f"âŒ Error checking Silero: {e}")
        return False

# ===== HÃ€M PHÃ‚N TÃCH THÃ”NG MINH =====
def smart_speaker_verification(enroll_file, verify_file, enroll_duration=30):
    print(f"ğŸ¯ SMART SPEAKER VERIFICATION")
    print(f"ğŸ“ Enroll: {enroll_file}")
    print(f"ğŸ“ Verify: {verify_file}")
    print("="*60)
    
    # 1. Táº¡o embedding reference
    print(f"ğŸ”„ Äang táº¡o embedding reference tá»« file: {enroll_file}")
    wav_check, sr_check = torchaudio.load(enroll_file)
    duration = wav_check.shape[1] / sr_check
    print(f"ğŸ“ Äá»™ dÃ i file enroll: {duration:.2f}s")

    max_duration = min(enroll_duration, duration - 1)
    enroll_emb = get_embedding(enroll_file, 1, max_duration)
    print(f"âœ… ÄÃ£ táº¡o embedding reference (shape: {enroll_emb.shape})")
    
    # 2. PhÃ¢n tÃ­ch file verify
    print(f"\nğŸ” Äang phÃ¢n tÃ­ch file: {verify_file}")
    wav, sr = torchaudio.load(verify_file)
    verify_duration = wav.shape[1] / sr
    print(f"ğŸ“ Äá»™ dÃ i file verify: {verify_duration:.2f}s")

    # Preprocessing
    if wav.shape[0] > 1:
        wav = wav[:1, :]
    if sr != 16000:
        wav = torchaudio.functional.resample(wav, sr, 16000)
        sr = 16000

    # 3. VAD vá»›i Silero VAD v6
    print("ğŸ”„ Loading Silero VAD v6...")
    silero_model, utils = torch.hub.load(
        repo_or_dir="snakers4/silero-vad",
        model="silero_vad",
        force_reload=False,
        onnx=False
    )
    (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
    
    # Sá»­ dá»¥ng tham sá»‘ tá»‘i Æ°u cho v6
    segments = get_speech_timestamps(
        wav.squeeze(), 
        silero_model, 
        sampling_rate=sr,
        threshold=0.5,                    # Threshold hÆ¡i cao hÆ¡n cho v6
        min_speech_duration_ms=1000,      # TÄƒng lÃªn 1s Ä‘á»ƒ lá»c tá»‘t hÆ¡n
        min_silence_duration_ms=200,      # Silence ngáº¯n hÆ¡n
        window_size_samples=1536,         # v6 default window size
        speech_pad_ms=30                  # Padding cho speech segments
    )

    print(f"ğŸ” VAD phÃ¡t hiá»‡n {len(segments)} Ä‘oáº¡n nÃ³i")
    
    # Optional: Sá»­ dá»¥ng VADIterator cho real-time processing (v6 feature)
    if len(segments) > 20:  # Chá»‰ dÃ¹ng streaming cho file dÃ i
        print("ğŸ“¡ Sá»­ dá»¥ng VADIterator cho streaming processing...")
        vad_iterator = VADIterator(silero_model, threshold=0.5, sampling_rate=sr)
        # Process audio in chunks for better memory usage
        chunk_size = sr * 30  # 30 seconds chunks
        refined_segments = []
        
        for i in range(0, wav.shape[1], chunk_size):
            chunk = wav[:, i:min(i+chunk_size, wav.shape[1])].squeeze()
            if len(chunk) > sr:  # Skip chunks < 1 second
                chunk_segments = vad_iterator(chunk)
                # Adjust timestamps for global position
                for seg in chunk_segments:
                    refined_segments.append({
                        'start': seg['start'] + i,
                        'end': seg['end'] + i
                    })
        
        vad_iterator.reset_states()  # v6 feature
        segments = refined_segments if refined_segments else segments
        print(f"ğŸ“¡ Streaming VAD Ä‘Ã£ tinh chá»‰nh: {len(segments)} Ä‘oáº¡n")
    
    # 4. TÃ­nh toÃ¡n cosine cho táº¥t cáº£ Ä‘oáº¡n
    cosine_scores = []
    segment_info = []
    
    for i, seg in enumerate(segments, 1):
        start_s = seg["start"] / sr
        end_s = seg["end"] / sr
        duration = end_s - start_s
        
        # Bá» qua Ä‘oáº¡n quÃ¡ ngáº¯n
        if duration < 1.0:  # TÄƒng threshold lÃªn 1s
            continue
        
        seg_wav = wav[:, seg["start"]:seg["end"]]
        emb = get_embedding_from_wav(seg_wav, sr)

        cos_sim = F.cosine_similarity(
            torch.tensor(enroll_emb).unsqueeze(0),
            torch.tensor(emb).unsqueeze(0)
        ).item()
        
        cosine_scores.append(cos_sim)
        segment_info.append((i, start_s, end_s, duration, cos_sim))
    
    if not cosine_scores:
        print("âš ï¸ KhÃ´ng cÃ³ Ä‘oáº¡n nÃ o Ä‘á»§ dÃ i Ä‘á»ƒ phÃ¢n tÃ­ch!")
        return
    
    # 5. Sá»­ dá»¥ng threshold cá»‘ Ä‘á»‹nh
    scores_array = np.array(cosine_scores)
    mean_score = scores_array.mean()
    std_score = scores_array.std()
    
    # Threshold cá»‘ Ä‘á»‹nh = 0.6
    dynamic_threshold = 0.6
    
    print(f"\nğŸ“Š PHÃ‚N TÃCH PHÃ‚N PHá»I:")
    print(f"   â€¢ Sá»‘ Ä‘oáº¡n há»£p lá»‡: {len(cosine_scores)}")
    print(f"   â€¢ Cosine trung bÃ¬nh: {mean_score:.3f}")
    print(f"   â€¢ Äá»™ lá»‡ch chuáº©n: {std_score:.3f}")
    print(f"   â€¢ Min/Max: {scores_array.min():.3f} / {scores_array.max():.3f}")
    print(f"   â€¢ Threshold cá»‘ Ä‘á»‹nh: {dynamic_threshold:.3f}")
    
    # 6. PhÃ¢n loáº¡i vá»›i threshold Ä‘á»™ng
    same_speaker = []
    different_speaker = []
    
    print(f"\nğŸ¯ Káº¾T QUáº¢ PHÃ‚N LOáº I:")
    for i, start_s, end_s, duration, cos_sim in segment_info:
        if cos_sim >= dynamic_threshold:
            label = "âœ… Giá»ng báº¡n"
            same_speaker.append((i, cos_sim))
        else:
            label = "âŒ NgÆ°á»i khÃ¡c" 
            different_speaker.append((i, cos_sim))
        
        print(f"Äoáº¡n {i}: {start_s:.1f}sâ€“{end_s:.1f}s ({duration:.1f}s) | Cosine={cos_sim:.3f} | {label}")
    
    # 7. Tá»•ng káº¿t
    print(f"\nğŸ“ˆ Tá»”NG Káº¾T:")
    print(f"   â€¢ Giá»ng báº¡n: {len(same_speaker)} Ä‘oáº¡n ({len(same_speaker)/len(segment_info)*100:.1f}%)")
    print(f"   â€¢ NgÆ°á»i khÃ¡c: {len(different_speaker)} Ä‘oáº¡n ({len(different_speaker)/len(segment_info)*100:.1f}%)")
    
    if same_speaker:
        same_scores = [score for _, score in same_speaker]
        print(f"   â€¢ Äá»™ tin cáº­y giá»ng báº¡n: {np.mean(same_scores):.3f} Â± {np.std(same_scores):.3f}")
    
    # 8. LÆ°u káº¿t quáº£
    np.save("enroll_emb.npy", enroll_emb)
    
    # LÆ°u segments Ä‘á»ƒ trÃ¡nh pháº£i cháº¡y VAD láº¡i
    segments_data = {
        'segments': segments,
        'file': verify_file,
        'sr': sr,
        'duration': verify_duration
    }
    np.save("vad_segments.npy", segments_data, allow_pickle=True)
    
    print(f"\nğŸ’¾ ÄÃ£ lÆ°u:")
    print(f"   â€¢ embedding reference: enroll_emb.npy")
    print(f"   â€¢ VAD segments: vad_segments.npy")
    
    return {
        'same_speaker_segments': same_speaker,
        'different_speaker_segments': different_speaker,
        'threshold': dynamic_threshold,
        'stats': {
            'mean': mean_score,
            'std': std_score,
            'min': scores_array.min(),
            'max': scores_array.max()
        }
    }

# ===== MAIN =====
if __name__ == "__main__":
    print("ğŸš€ ENHANCED SPEAKER VERIFICATION WITH SILERO VAD v6")
    print("="*60)
    
    # Kiá»ƒm tra Silero VAD version
    is_v6 = check_silero_version()
    
    # Cáº¥u hÃ¬nh
    enroll_file = "Viá»‡t Anh_24.9.wav"
    verify_file = "conversation-test.wav"
    
    print(f"\nğŸ“ Files:")
    print(f"   â€¢ Enroll: {enroll_file}")
    print(f"   â€¢ Verify: {verify_file}")
    print("="*60)
    
    # Cháº¡y phÃ¢n tÃ­ch
    result = smart_speaker_verification(enroll_file, verify_file, enroll_duration=30)
    
    print(f"\nğŸ”§ KHUYáº¾N NGHá»Š ÄIá»€U CHá»ˆNH:")
    print(f"   â€¢ Threshold hiá»‡n táº¡i: {result['threshold']:.3f} (cá»‘ Ä‘á»‹nh)")
    print(f"   â€¢ Náº¿u quÃ¡ nhiá»u false positive: TÄƒng lÃªn 0.70-0.75")
    print(f"   â€¢ Náº¿u quÃ¡ nhiá»u false negative: Giáº£m xuá»‘ng 0.60-0.65")
    print(f"   â€¢ Threshold Ä‘á»™ng sáº½ lÃ : {result['stats']['mean'] + 0.2 * result['stats']['std']:.3f}")